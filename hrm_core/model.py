from typing import Dict, Optional, Tuple
import torch
import torch.nn as nn
from .config import HRMCoreConfig
from .inner import HRMCoreInner


class HRMForQA(nn.Module):
    """HRM‑based extractive reader with gated fusion + soft‑halting."""
    def __init__(self, cfg: HRMCoreConfig):
        super().__init__()
        self.cfg = cfg

        # Embeddings (có thể thay bằng external encoder outputs)
        self.token_emb = nn.Embedding(cfg.vocab_size, cfg.hidden_size)
        self.pos_emb = nn.Embedding(cfg.max_position_embeddings, cfg.hidden_size)
        self.ln_in = nn.LayerNorm(cfg.hidden_size)

        # Hierarchical core
        self.inner = HRMCoreInner(cfg)

        # Span head
        self.qa_head = nn.Linear(cfg.hidden_size, 2)

    def init_states(self, batch_size: int, seq_len: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
        H0 = self.inner.H_init.expand(batch_size, seq_len, -1).to(device)
        L0 = self.inner.L_init.expand(batch_size, seq_len, -1).to(device)
        return H0, L0

    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,  # 1=keep, 0=pad
        start_positions: Optional[torch.Tensor] = None,
        end_positions: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> Dict[str, torch.Tensor]:
        if inputs_embeds is not None:
            B, S = inputs_embeds.size(0), inputs_embeds.size(1)
            device = inputs_embeds.device
            x = self.ln_in(inputs_embeds)
        else:
            assert input_ids is not None
            B, S = input_ids.size(0), input_ids.size(1)
            device = input_ids.device
            pos = torch.arange(S, device=device).unsqueeze(0).expand(B, S)
            x = self.token_emb(input_ids) + self.pos_emb(pos)
            x = self.ln_in(x)

        # Init states
        z_H, z_L = self.init_states(B, S, device)

        # Convert HF‑style attention_mask -> key_padding_mask (True means masked)
        key_padding_mask = None
        if attention_mask is not None:
            key_padding_mask = (attention_mask == 0)

        # Hierarchical reasoning với soft‑halting
        z_H_out, z_L_last = self.inner(z_H, z_L, token_embeddings=x, key_padding_mask=key_padding_mask)

        # Span head trên H_out
        logits = self.qa_head(z_H_out)  # [B, S, 2]
        start_logits, end_logits = logits[..., 0], logits[..., 1]

        out = {"start_logits": start_logits, "end_logits": end_logits}

        if start_positions is not None and end_positions is not None:
            ignored_index = start_logits.size(1)
            start_positions = start_positions.clamp(0, ignored_index)
            end_positions = end_positions.clamp(0, ignored_index)

            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)
            start_loss = loss_fct(start_logits, start_positions)
            end_loss = loss_fct(end_logits, end_positions)
            out["loss"] = (start_loss + end_loss) / 2

        return out